

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object WordCount{

    def main(args:Array[String]):Unit={
        val conf=new SparkConf()
                .setAppName("WordCount")
                .setMaster("local[*]")
        val sc=new SparkContext(conf)
        val textFile=sc.textFile(args(0))

        val counts=textFile
                    .flatMap(line=>line.split("\\s+"))
                    .map(word=>(word.toLowerCase,1))
                    .reduceByKey(_+_)
        
        
        counts.saveAsTextFile(args(1))

        sc.stop()
    }

}








find /usr -name "hadoop*sources*.jar"
find ~ -name "hadoop*sources*.jar"

////////////Copy path

start-dfs.sh
start-yarn.sh
jps

unzip path(of jar) OR jar xf path
ls  //check org
cd /org/apache/hadoop/...


cp (path of wordcount.java) Mock(path)
cd Mock
nano WordCount.java
// copy paste remove license and packages


javac -cp $(hadoop classpath) WordCount.java
jar cf WordCount.jar WordCount*.class
hdfs dfs -rm -r /input /output
hdfs dfs -mkdir /input
hdfs dfs -put ./input.txt /input
hadoop jar WordCount.jar WordCount /input/input.txt /output
hdfs dfs -cat /output/part-r-00000










code


spark-shell -i WordCount.scala(path) 



echo $SPARK_HOME                /// ~/ .bashrc
scalac -cp 'path/jars/*' WordCount.scala
jar cf WordCount.jar WordCount*.class
spark-submit --class WordCount WordCount.jar file://iput(path) file://output(path)





Sure! Let‚Äôs break it down clearly and systematically:

---

# üìö What is Hadoop?

**Hadoop** is an **open-source framework** designed to store and process **large volumes of data** (big data) across many computers in a **distributed** and **fault-tolerant** manner.

‚úÖ It handles structured, semi-structured, and unstructured data.

‚úÖ It is scalable (can add more computers easily).

‚úÖ It is cost-effective (runs on commodity hardware).

---

# üèõÔ∏è Hadoop Architecture Overview

Hadoop mainly consists of two core components:

1. **HDFS (Hadoop Distributed File System)** ‚Üí for **storage**.
2. **MapReduce** ‚Üí for **processing**.

Besides these, it also has **YARN (Yet Another Resource Negotiator)** to manage cluster resources.

The architecture has mainly **two types of nodes**:

| Node Type | Function |
|:----------|:---------|
| **Master Node** | Controls and manages the system. |
| **Slave Node** | Stores actual data and performs computations. |

---

# üñ•Ô∏è Components and Their Working

## 1. **HDFS (Storage System)**

- **Master Node ‚Üí NameNode**
  - Manages the **filesystem namespace**.
  - Keeps **metadata** (file names, locations of blocks, permissions).
  - Does **not store the actual data** ‚Äî only information **about** data.

- **Slave Nodes ‚Üí DataNodes**
  - Actually **store the data** (block files).
  - Send periodic heartbeats to NameNode to tell they're alive.
  - Handle requests like **read**, **write**, **replication**.

- **Secondary NameNode**
  - It is **not** a backup NameNode.
  - It **periodically merges** the NameNode‚Äôs metadata (edit logs + fsimage) to prevent memory overflow.
  - Assists in failure recovery **but can't** fully replace NameNode immediately.

---

## 2. **YARN (Resource Management System)**

- **ResourceManager (Master)**
  - Manages **resources** across all nodes.
  - Handles **job scheduling**.
  
- **NodeManager (Slave)**
  - Runs on each machine.
  - Manages **containers** (execution environments).
  - Reports node health and resource usage to ResourceManager.

---

## 3. **MapReduce (Processing Framework)**

- Divides jobs into **Map tasks** and **Reduce tasks**.
- **Map** Phase:
  - Input data is split and processed into (key, value) pairs.
- **Shuffle and Sort** Phase:
  - Intermediate data is transferred and sorted between Map and Reduce.
- **Reduce** Phase:
  - Aggregates results from Map phase.

> Example: Counting the number of occurrences of words in a file.

---

# üîµ How the Nodes Work Together (Simplified Example):

1. **Client** submits a file.
2. **NameNode** splits the file into **blocks** (default 128 MB) and decides which **DataNodes** to store these blocks.
3. Blocks are stored **with replication** (usually 3 copies) on different **DataNodes**.
4. **ResourceManager** accepts the job request and assigns work to **NodeManagers**.
5. **NodeManagers** launch **Map tasks** and **Reduce tasks** in containers.
6. After processing, results are sent back to the **client**.

---

# üéØ Why is Hadoop Used? (Importance)

| Reason | Explanation |
|:-------|:------------|
| **Handles Big Data** | Can process petabytes (even exabytes) of data. |
| **Fault Tolerant** | If one node fails, Hadoop retrieves data from other replicated copies. |
| **Scalable** | Easily add more machines to the cluster. |
| **Cost-Effective** | Runs on normal, cheap hardware (no need for supercomputers). |
| **Flexible** | Can store any type of data (text, images, videos, logs, etc.). |
| **Open-source** | Free to use and backed by a strong community. |

---

# üî• Diagram to Visualize Hadoop Architecture
```
+---------------------------+
|         Client            |
+---------------------------+
           |
        Submit Job
           ‚Üì
+---------------------------+
|         NameNode          | ‚Üê (HDFS - Master)
+---------------------------+
        /        \
+------------+  +------------+
| DataNode 1 |  | DataNode 2 |  ... (HDFS - Slaves)
+------------+  +------------+

+---------------------------+
|      ResourceManager      | ‚Üê (YARN - Master)
+---------------------------+
        /        \
+------------+  +------------+
| NodeManager|  | NodeManager|  ... (YARN - Slaves)
+------------+  +------------+
```

---

# üìù Quick Summary

| Component | Type | Responsibility |
|:---------|:-----|:----------------|
| NameNode | Master | Manages metadata (HDFS) |
| DataNode | Slave | Stores actual data blocks |
| ResourceManager | Master | Manages resources and job scheduling |
| NodeManager | Slave | Manages job execution on a single node |
| Secondary NameNode | Helper | Helps NameNode in checkpointing |

---

Would you also like me to give you a **real-world example** (like how Facebook or Amazon uses Hadoop)? üöÄ It helps in remembering it better! üéØ


















Good question! Let's dive into **how Hadoop manages replication** of data inside **HDFS (Hadoop Distributed File System)**:

---

# üîµ How Hadoop Manages Replication

When a file is stored in HDFS, it is divided into **blocks** (typically **128 MB** or **256 MB** per block).  
Each block is **replicated** (by default **3 copies**) across different **DataNodes**.

This ensures **fault tolerance** ‚Äî if one node goes down, the data is **still available** from another copy.

---

# ‚öôÔ∏è Detailed Working of Replication

### 1. **During File Write (Initial Replication)**

- The client contacts the **NameNode** to write a file.
- The **NameNode**:
  - Splits the file into blocks.
  - Decides **which DataNodes** will store each block **and its replicas**.
- Data is written like a **pipeline**:
  - The client writes the first replica to **DataNode A**.
  - **DataNode A** then forwards the block to **DataNode B**.
  - **DataNode B** forwards to **DataNode C**.
- So the **client only connects to the first DataNode**, and the rest happens internally.

This is called **pipelined replication**. üö∞

---

### 2. **Choosing DataNodes for Replicas (Rack Awareness)**

Hadoop tries to place replicas smartly across the cluster:

| Replica No. | Placement |
|:------------|:----------|
| 1st Replica | On a node in the local rack (where the client is writing from) |
| 2nd Replica | On a node in a different rack |
| 3rd Replica | On a different node in the same remote rack |

‚úÖ This design makes sure that **even if a full rack fails**, data is not lost.

---
  
### 3. **Replication Factor**

- **Default replication factor**: **3** (can be configured).
- You can set different replication factors **per file** or **per directory**.

Example command to set replication:
```bash
hdfs dfs -setrep -w 2 /path/to/file
```
This sets replication **to 2** for that file.

---

### 4. **Replication Monitoring and Maintenance**

- **DataNodes** send regular **heartbeat signals** to the **NameNode**.
- If a **DataNode fails**:
  - NameNode notices missing replicas.
  - It chooses **other healthy DataNodes**.
  - It **automatically replicates** missing blocks to maintain the desired replication factor.

‚úÖ **Self-healing mechanism**.

---

# üõ†Ô∏è Example

Suppose you have a 400 MB file, block size is 128 MB, replication factor is 3:

| Block | Size | Replicas |
|:------|:-----|:---------|
| Block 1 | 128 MB | 3 replicas |
| Block 2 | 128 MB | 3 replicas |
| Block 3 | 128 MB | 3 replicas |
| Block 4 | 16 MB  | 3 replicas |

Total **number of block copies** stored = **(4 blocks √ó 3 replicas) = 12 block copies**.

So the actual storage consumed = much more than just 400 MB ‚Äî it‚Äôs **400 MB √ó 3** approximately.

---

# üß† Important Properties

| Feature | Detail |
|:--------|:-------|
| **High availability** | Because of multiple replicas |
| **Rack awareness** | Spread replicas across racks |
| **Automatic replication** | If node fails, new replicas are created |
| **Configurable replication** | Can be tuned as needed |

---

# ‚ú® Quick Diagram

```
File: "data.txt" ‚Üí split into 2 blocks (Block A and Block B)

+---------------------------+
|        NameNode           |
|  - Tracks where blocks are|
|  - Handles replication    |
+---------------------------+
         /        \
        ‚Üì          ‚Üì
+----------+   +----------+
| DataNode1|   | DataNode2|   (Block A copy)
+----------+   +----------+
        ‚Üì           ‚Üì
+----------+   +----------+
| DataNode3|   | DataNode4|   (Block B copy)
+----------+   +----------+
```

If **DataNode2** fails, NameNode will **re-replicate** Block A to another healthy DataNode.

---

# üöÄ Quick Summary
- Hadoop keeps **multiple copies** of each data block.
- Replication ensures **fault tolerance** and **availability**.
- Replicas are **strategically placed** across racks (Rack Awareness).
- Replication is **automatically monitored and repaired** by the **NameNode**.

---

Would you like me also to explain **what happens during a "block corruption" case** and **how Hadoop handles it**? üöÄ (It's a cool internal working!)




















Of course! Let's explain **Apache Spark architecture** just like we did for Hadoop ‚Äî clearly and step-by-step:

---

# üìö What is Apache Spark?

**Spark** is a **fast**, **open-source**, **distributed computing** system used for **big data processing**.

‚úÖ It processes data **in-memory** (in RAM) ‚Äî much faster than Hadoop's disk-based MapReduce.

‚úÖ It supports **batch processing**, **stream processing**, **machine learning**, **SQL queries**, and even **graph processing**.

‚úÖ It can run **on top of Hadoop**, standalone, or even on cloud clusters.

---

# üèõÔ∏è Spark Architecture Overview

Spark architecture mainly has **two types of nodes**:

| Node Type | Role |
|:----------|:-----|
| **Driver** | Master node controlling the execution |
| **Worker** | Slave nodes that perform tasks |

Internally, it also uses:

- **Cluster Manager** ‚Üí Manages resources across machines.
- **Executors** ‚Üí Processes launched on Workers.
- **Tasks** ‚Üí Actual units of work.

---

# üñ•Ô∏è Components and Their Working

## 1. **Driver Program (Master)**

- Runs the **main()** function of the Spark application.
- Creates a **SparkContext** (the entry point of Spark).
- **Plans** jobs into **tasks** and **schedules** them.
- Coordinates between all the **executors**.
- Maintains information about the **RDDs (Resilient Distributed Datasets)**, transformations, and actions.

‚úÖ Think of it as the "brain" of your Spark job.

---

## 2. **Cluster Manager**

- Manages the cluster resources.
- Spark can work with different cluster managers:
  - **Standalone** (built-in Spark manager)
  - **Apache YARN** (same as Hadoop's resource manager)
  - **Apache Mesos**
  - **Kubernetes**

‚úÖ It tells Spark where it can launch Executors.

---

## 3. **Worker Nodes**

- These are the **machines** where the real computation happens.
- Each Worker node runs:
  - One or more **Executors**.

---

## 4. **Executors**

- Executors **run on Worker nodes**.
- They are **responsible for**:
  - **Executing tasks** assigned by the Driver.
  - **Storing data** (in-memory or disk) for **caching** (important for speeding up iterative algorithms like ML).
- Each Spark application has its own set of Executors.

‚úÖ If an Executor dies, Spark can relaunch it (depending on settings).

---

## 5. **Tasks**

- A **task** is the **smallest unit of work** in Spark.
- Each job is **split into multiple tasks** based on data partitions.
- Executors run the tasks.

---

# üîµ How the Nodes Work Together (Simplified Example):

1. The user submits a Spark application (say, a Python or Scala program).
2. The **Driver** creates a **SparkContext** and requests resources from the **Cluster Manager**.
3. The **Cluster Manager** allocates **Workers** to run **Executors**.
4. The Driver **divides a job into multiple tasks**.
5. These **tasks are sent to Executors**.
6. **Executors** perform the computation and send the results back to the **Driver**.

---

# ‚ú® Quick Diagram

```
+------------------+
|   Driver Program  |
| (SparkContext)    |
+------------------+
         |
         ‚Üì
+--------------------------+
|   Cluster Manager        |
| (Standalone, YARN, etc.) |
+--------------------------+
         |
         ‚Üì
+--------------+      +--------------+
| Worker Node 1|      | Worker Node 2 |
| + Executor   |      | + Executor    |
+--------------+      +--------------+
         ‚Üì                  ‚Üì
    Runs Tasks          Runs Tasks
```

---

# ‚ö° Key Concept: RDD (Resilient Distributed Dataset)

- Fundamental data structure in Spark.
- **Immutable** distributed collection of objects.
- RDDs are split into **partitions** and processed **in parallel**.

‚úÖ Fault-tolerant ‚Äî if a node fails, RDD can be recomputed using **lineage information**.

---

# üéØ Why is Spark Used? (Importance)

| Reason | Explanation |
|:-------|:------------|
| **In-Memory Computation** | Data stored in RAM ‚Üí faster than Hadoop MapReduce (which reads/writes disk). |
| **Fault Tolerant** | RDDs remember how to recreate data if a node fails. |
| **Versatile** | Supports SQL, Streaming, Machine Learning (MLlib), Graph processing (GraphX). |
| **Compatible** | Can run on Hadoop (YARN), Kubernetes, Standalone. |
| **Easy to Use** | Supports Java, Scala, Python, R APIs. |

---

# üß† Example Flow (WordCount)

Suppose you want to count the number of times each word appears in a file:

1. **Driver** reads input file ‚Üí creates RDD.
2. RDD is split into partitions.
3. Each **Executor** counts word frequencies for its partition (Map Phase).
4. Results are combined (Reduce Phase).
5. Final count is collected back to the **Driver**.

‚úÖ All major computation happens in Executors.

---

# üõ†Ô∏è Quick Comparison: Hadoop vs Spark

| Feature | Hadoop MapReduce | Spark |
|:--------|:-----------------|:------|
| **Speed** | Slow (disk-based) | Fast (in-memory) |
| **Processing** | Batch only | Batch + Streaming |
| **Ease of use** | More complex | Simpler APIs |
| **Fault Tolerance** | Replication-based | Lineage-based |
| **Best for** | Massive archival jobs | Real-time and iterative jobs |

---

# üöÄ Quick Summary

| Component | Type | Responsibility |
|:---------|:-----|:----------------|
| Driver | Master | Controls job flow and task scheduling |
| Cluster Manager | Master | Allocates resources |
| Worker | Slave | Hosts executors |
| Executor | Worker process | Runs tasks and stores data |
| Task | Work unit | Smallest execution unit |

---

Would you also like me to quickly explain **how Spark handles failures** (e.g., if an Executor crashes)? It‚Äôs quite interesting how **RDD lineage** helps here! üöÄ Let me know!


































Alright! Let's go **in detailed explanation** ‚Äî  
I'll explain each plot **deeply** with:

- üìö **What is it exactly?**
- üî• **Why and when to use?**
- ‚öôÔ∏è **How it works internally?**
- üßπ **Syntax in Matplotlib/Seaborn**
- üß† **Example situations**

---

# üìà 1. Line Plot

### üìö What:
A **Line plot** connects individual data points with a line ‚Äî simple but powerful for trends.

### üî• When to Use:
- Time-series data (stock price, temperature changes).
- Observing increasing/decreasing patterns.

### ‚öôÔ∏è How it works:
- X-axis: independent variable (e.g., time).
- Y-axis: dependent variable (e.g., stock price).
- Line connects the points chronologically.

### üßπ Syntax:
```python
import matplotlib.pyplot as plt
plt.plot(x, y)
plt.title("Stock Price over Time")
plt.xlabel("Date")
plt.ylabel("Price")
plt.show()
```

### üß† Example:
- Temperature variation during a day.
- COVID-19 cases over months.

---

# üìä 2. Bar Plot

### üìö What:
A **Bar plot** represents categorical data with rectangular bars.

### üî• When to Use:
- Comparing different groups/categories.
- Summarizing survey results.

### ‚öôÔ∏è How it works:
- Each bar‚Äôs height = value/count of category.

### üßπ Syntax:
```python
import seaborn as sns
sns.barplot(x='city', y='population', data=df)
```

### üß† Example:
- Comparing profits of different companies.
- Count of students in different branches.

---

# üìà 3. Histogram

### üìö What:
A **Histogram** shows the **distribution** of a continuous variable.

### üî• When to Use:
- Understanding how values are spread (normal, skewed, bimodal).

### ‚öôÔ∏è How it works:
- Divides data into bins (ranges).
- Counts how many values fall into each bin.

### üßπ Syntax:
```python
plt.hist(data, bins=20)
```

### üß† Example:
- Distribution of people's ages.
- Distribution of salaries.

---

# üì¶ 4. Box Plot

### üìö What:
A **Box plot** (or Whisker plot) summarizes a dataset‚Äôs minimum, Q1, median, Q3, and maximum ‚Äî shows outliers.

### üî• When to Use:
- Finding outliers.
- Checking data symmetry/skewness.

### ‚öôÔ∏è How it works:
- Draws a box from Q1 to Q3.
- Line inside = Median.
- Whiskers extend to min and max (excluding outliers).

### üßπ Syntax:
```python
sns.boxplot(x='department', y='salary', data=df)
```

### üß† Example:
- Finding outliers in exam scores.
- Salary comparison between departments.

---

# üéØ 5. Scatter Plot

### üìö What:
A **Scatter plot** shows the relationship between two variables.

### üî• When to Use:
- Identifying correlation (positive, negative, none).
- Finding clusters or patterns.

### ‚öôÔ∏è How it works:
- Each point = one observation (x, y).

### üßπ Syntax:
```python
plt.scatter(x, y)
```
or
```python
sns.scatterplot(x='feature1', y='feature2', data=df)
```

### üß† Example:
- Height vs Weight analysis.
- Study time vs Exam scores.

---

# üî• 6. Heatmap

### üìö What:
A **Heatmap** uses color shades to represent values ‚Äî often used to visualize correlation matrices.

### üî• When to Use:
- Understanding feature relationships.
- Multivariable comparisons.

### ‚öôÔ∏è How it works:
- Each cell color intensity = value magnitude.

### üßπ Syntax:
```python
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
```

### üß† Example:
- Visualizing correlation between features in a dataset.

---

# ü•ß 7. Pie Chart

### üìö What:
A **Pie chart** shows parts of a whole as slices.

### üî• When to Use:
- Representing proportions (limited categories).

### ‚öôÔ∏è How it works:
- Each category gets a slice proportional to its share.

### üßπ Syntax:
```python
plt.pie(values, labels=categories, autopct='%1.1f%%')
```

### üß† Example:
- Market share of companies.
- Population distribution by religion.

---

# ü§ù 8. Pair Plot

### üìö What:
A **Pair plot** shows scatter plots between all pairs of numerical columns.

### üî• When to Use:
- Exploring relationships across many features quickly.

### ‚öôÔ∏è How it works:
- Diagonal: histograms
- Off-diagonal: scatter plots

### üßπ Syntax:
```python
sns.pairplot(df, hue='target')
```

### üß† Example:
- Iris dataset exploration.

---

# üéª 9. Violin Plot

### üìö What:
A **Violin plot** combines a box plot and KDE plot.

### üî• When to Use:
- Comparing data distributions between categories.

### ‚öôÔ∏è How it works:
- Symmetrical KDE (density) plot around the boxplot.

### üßπ Syntax:
```python
sns.violinplot(x='group', y='value', data=df)
```

### üß† Example:
- Distribution of marks across sections.

---

# üìä 10. Count Plot

### üìö What:
A **Count plot** counts the number of instances in each category.

### üî• When to Use:
- Display categorical frequency.

### ‚öôÔ∏è How it works:
- Like a bar plot but automatically counts from the data.

### üßπ Syntax:
```python
sns.countplot(x='category', data=df)
```

### üß† Example:
- Number of people in different occupations.

---

# üìà 11. Dist Plot

### üìö What:
A **Dist plot** shows histogram + smooth density curve.

### üî• When to Use:
- Understanding both distribution + probability.

### ‚öôÔ∏è How it works:
- Overlays a KDE line over histogram bars.

### üßπ Syntax:
```python
sns.histplot(data, kde=True)
```

### üß† Example:
- Salary distributions.

---

# üìà 12. Area Plot

### üìö What:
An **Area plot** is like a filled line plot ‚Äî shows cumulative totals.

### üî• When to Use:
- Showing stacking data over time.

### ‚öôÔ∏è How it works:
- Plots line plots but fills below them.

### üßπ Syntax:
```python
df.plot.area()
```

### üß† Example:
- Cumulative sales per quarter.

---

# üß≠ 13. 3D Surface Plot

### üìö What:
A **3D Surface plot** shows relationships between three variables (X, Y, Z).

### üî• When to Use:
- Visualizing 3D relationships.
- Understanding optimization landscapes.

### ‚öôÔ∏è How it works:
- Mesh of x, y points, heights at z-values.

### üßπ Syntax:
```python
from mpl_toolkits.mplot3d import Axes3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot_surface(X, Y, Z, cmap='viridis')
plt.show()
```

### üß† Example:
- Surface temperature modeling.

---

# üìä 14. Stacked Bar Plot

### üìö What:
A **Stacked Bar plot** shows multiple layers inside a bar.

### üî• When to Use:
- Showing part-to-whole across different groups.

### ‚öôÔ∏è How it works:
- Each bar is divided into sub-parts stacked on top.

### üßπ Syntax:
```python
plt.bar(x, A)
plt.bar(x, B, bottom=A)
```

### üß† Example:
- Population by age and gender.

---

# ‚ú® BONUS ‚Äî Interactive Plots (Plotly)

If you want hover labels, zoom, drag, tooltips:
```python
import plotly.express as px
fig = px.scatter(df, x="feature1", y="feature2", color="target")
fig.show()
```

Plotly plots are **interactive** unlike Matplotlib and Seaborn.

---

# üéØ Summary Diagram (quick memory)

| Plot         | Best for                          | Package |
|--------------|------------------------------------|---------|
| Line Plot    | Trends over time                   | Matplotlib |
| Bar Plot     | Categorical comparison             | Seaborn |
| Histogram    | Data distribution                  | Matplotlib |
| Box Plot     | Spread, outliers                   | Seaborn |
| Scatter Plot | Relationship between 2 variables   | Seaborn |
| Heatmap      | Correlation matrix                 | Seaborn |
| Pie Chart    | Percentages                        | Matplotlib |
| Pair Plot    | Multi-variable relationships       | Seaborn |
| Violin Plot  | Distributions + shape              | Seaborn |
| Count Plot   | Categorical counts                 | Seaborn |
| Dist Plot    | Distribution + density             | Seaborn |
| Area Plot    | Cumulative data over time          | Pandas |
| 3D Surface   | 3-variable visualization           | Matplotlib 3D |
| Stacked Bar  | Subcategories inside bars          | Matplotlib |

---

Would you also like me to share a **PDF cheat-sheet** that you can download and keep offline üìÑ?  
(With tiny diagrams beside each plot for quick revision) üéØ  
Shall I prepare that too? üöÄ








